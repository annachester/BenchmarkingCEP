{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0235b5d2",
   "metadata": {},
   "source": [
    "## Throughput data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5888c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fa055a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for log files, works for pattern and query\n",
    "def get_throughput_df_from_log(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = file.readlines()\n",
    "        \n",
    "    run_ids = []\n",
    "    worker_ids = []\n",
    "    elements_per_second = []\n",
    "    mb_per_sec = []\n",
    "    \n",
    "    settings = {}#\"file_loops\", \"vel_thresh\", \"qua_thresh\", \"window\", \"iteration\", \"throughput\", \"workers\"}\n",
    "    \n",
    "    pattern = r\"\\$(.*?)\\$\"\n",
    "    linecount = 0\n",
    "    runcount = 0\n",
    "    for line in data:\n",
    "        \n",
    "        if \"util.ThroughputLogger\" in line:\n",
    "            values = re.findall(pattern, line)\n",
    "            \n",
    "            if \"file loops\" in line: #first line with settings\n",
    "                settings[\"name\"] = values[0]\n",
    "                settings[\"file_loops\"] = int(values[1])\n",
    "                settings[\"vel_thresh\"] = int(values[2])\n",
    "                if values[3] != \"null\":\n",
    "                    settings[\"qua_thresh\"] = int(values[3])\n",
    "                settings[\"window\"] = int(values[4])\n",
    "                if values[5] != \"null\":\n",
    "                    settings[\"iteration\"] = int(values[5])\n",
    "            \n",
    "            elif \"sensors\" in line: #optional line for parallel execution\n",
    "                settings[\"sensors\"] = int(values[0])\n",
    "                \n",
    "            elif \"throughput\" in line: #second line\n",
    "                settings[\"throughput\"] = int(values[0])\n",
    "                settings[\"workers\"] = int(values[1])\n",
    "            \n",
    "            elif \"elements/second\" in line: #all other lines\n",
    "                linecount += 1\n",
    "                #this line may be ueberfluessig\n",
    "                run_ids.append(runcount)\n",
    "                worker_ids.append(int(values[0]))\n",
    "                elements_per_second.append(int(values[1]))\n",
    "                mb_per_sec.append(float(values[2]))\n",
    "\n",
    "                if linecount % settings[\"workers\"] == 0:\n",
    "                    runcount += 1\n",
    "    df = pd.DataFrame({\n",
    "        \"index\": run_ids,\n",
    "        \"Worker ID\": worker_ids,\n",
    "        \"Elements/Second\": elements_per_second,\n",
    "        \"MB/Second\": mb_per_sec\n",
    "    })\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a47114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_settings_from_log(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = file.readlines()\n",
    "        \n",
    "    settings = {}\n",
    "    \n",
    "    for line in data:\n",
    "        if \"util.ThroughputLogger\" in line:\n",
    "            pattern = r\"\\$(.*?)\\$\"\n",
    "            values = re.findall(pattern, line)\n",
    "            if \"file loops\" in line: #first line with settings\n",
    "                settings[\"name\"] = make_nice_name(values[0])\n",
    "                settings[\"file_loops\"] = int(values[1])\n",
    "                settings[\"vel_thresh\"] = int(values[2])\n",
    "                if values[3] != \"null\":\n",
    "                    settings[\"qua_thresh\"] = int(values[3])\n",
    "                settings[\"window\"] = int(values[4])\n",
    "                if values[5] != \"null\":\n",
    "                    settings[\"iteration\"] = int(values[5])\n",
    "            \n",
    "            elif \"sensors\" in line: #optional line for parallel execution\n",
    "                settings[\"sensors\"] = int(values[0])\n",
    "                \n",
    "            elif \"throughput\" in line: #second line\n",
    "                settings[\"throughput\"] = int(values[0])\n",
    "                settings[\"workers\"] = int(values[1])\n",
    "                \n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23c261e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nice_name(name):\n",
    "    if name.endswith(\"LS\"):\n",
    "        name = name.split(\"LS\")[0]\n",
    "        \n",
    "    if \"Pattern\" in name:\n",
    "        nice_name = \"FlinkCEP_\"\n",
    "    elif \"Query\" in name:\n",
    "        nice_name = \"CEP2ASP_\"\n",
    "        \n",
    "    if \"ITER1\" in name:\n",
    "        nice_name += \"ITER1\"\n",
    "    elif \"ITER2\" in name:\n",
    "        nice_name += \"ITER2\"\n",
    "    \n",
    "    elif \"AND\" in name:\n",
    "        nice_name += \"AND\"\n",
    "    elif \"OR\" in name:\n",
    "        nice_name += \"OR\"    \n",
    "    \n",
    "    elif \"Q1_SEQ\" in name:\n",
    "        nice_name += \"SEQ1-SP0\"\n",
    "    elif \"Q1_1_SEQ\" in name:\n",
    "        nice_name += \"SEQ1-SP1\"\n",
    "    elif \"Q1_2_SEQ\" in name:\n",
    "        nice_name += \"SEQ1-SP2\"\n",
    "        \n",
    "    elif \"Q8_SEQ\" in name:\n",
    "        nice_name += \"SEQ2-SP0\"\n",
    "    elif \"Q8_1_SEQ\" in name:\n",
    "        nice_name += \"SEQ2-SP1\"\n",
    "    elif \"Q8_2_SEQ\" in name:\n",
    "        nice_name += \"SEQ2-SP2\"\n",
    "    \n",
    "    return nice_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbd9eed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlinkCEP-SEQ2-SP2\n"
     ]
    }
   ],
   "source": [
    "#print(make_nice_name(\"Q8_2_SEQPatternLS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b3eb983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use for logs saved to csv, this approach was not used in the end because of slow writing\n",
    "def get_throughput_df_from_csv(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = file.readlines()\n",
    "\n",
    "\n",
    "    # Define empty lists to store extracted information\n",
    "    run_ids = []\n",
    "    worker_ids = []\n",
    "    elements_per_second = []\n",
    "    mb_per_sec = []\n",
    "\n",
    "    # Regular expression pattern to extract numerical values from the lines surrounded by $\n",
    "    pattern = r\"\\$(.*?)\\$\"\n",
    "\n",
    "    runcount = 0\n",
    "    linecount = 0\n",
    "    data_ingestion_rate = 0\n",
    "    number_of_workers = 0\n",
    "    for line in data:\n",
    "        values = re.findall(pattern, line)\n",
    "\n",
    "        if not line.startswith(\"Worker\"): #header lines\n",
    "            data_ingestion_rate = int(values[0])\n",
    "            number_of_workers = int(values[1])\n",
    "            \n",
    "        else:\n",
    "            if len(values) == 3:\n",
    "                linecount += 1\n",
    "                worker_id = int(values[0])\n",
    "                elements_per_second_value = float(values[1])\n",
    "                mb_per_sec_value = float(values[2])\n",
    "\n",
    "                run_ids.append(runcount)\n",
    "                worker_ids.append(worker_id)\n",
    "                elements_per_second.append(elements_per_second_value)\n",
    "                mb_per_sec.append(mb_per_sec_value)\n",
    "\n",
    "                if linecount % number_of_workers == 0:\n",
    "                    runcount += 1\n",
    "\n",
    "    # Create a pandas DataFrame from the extracted data\n",
    "    df = pd.DataFrame({\n",
    "        \"index\": run_ids,\n",
    "        \"Worker ID\": worker_ids,\n",
    "        \"Elements/Second\": elements_per_second,\n",
    "        \"MB/Second\": mb_per_sec\n",
    "    })\n",
    "    return df, number_of_workers, data_ingestion_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26062c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_throughput(df):\n",
    "    Elements_sec_worker = df[\"Elements_sec_worker\"].mean()\n",
    "    Total_elements_sec = df[\"Total_elements_sec\"].mean()\n",
    "    MB_sec_worker = df[\"MB_sec_worker\"].mean()\n",
    "    return np.array([Elements_sec_worker, Total_elements_sec, MB_sec_worker])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d200318",
   "metadata": {},
   "source": [
    "## Latency data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78097f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latency_df_from_log(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = file.readlines()\n",
    "\n",
    "    event_det_lat = []\n",
    "    pattern_det_lat = []\n",
    "    total_latency = []\n",
    "    patterns_cnt = []\n",
    "\n",
    "    # Regular expression pattern to extract numerical values from the lines surrounded by $\n",
    "    pattern = r\"\\$(.*?)\\$\"\n",
    "\n",
    "    for line in data:\n",
    "        if \"util.LatencyLogger\" in line:\n",
    "            values = re.findall(pattern, line)\n",
    "\n",
    "            if len(values) == 4: \n",
    "                # Append the extracted data to the lists\n",
    "                event_det_lat.append(int(values[0]))\n",
    "                pattern_det_lat.append(int(values[1]))\n",
    "                total_latency.append(int(values[2]))\n",
    "                patterns_cnt.append(int(values[3]))\n",
    "#             else:\n",
    "#                 print(line) #header\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"event_det_lat\": event_det_lat,\n",
    "        \"pattern_det_lat\": pattern_det_lat,\n",
    "        \"total_latency\": total_latency,\n",
    "        \"patterns_cnt\": patterns_cnt\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "806e7228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use for logs saved to csv, this approach was not used in the end because of slow writing\n",
    "#used for CEP2ASP approach, gets unique result, since approach outputs multiple duplicates\n",
    "def create_latency_df_query_unique(file_path):\n",
    "    \n",
    "    latency_path = file_path+\"/latency.csv\"\n",
    "    result_path = file_path+\"/result_tuples.csv\"\n",
    "    \n",
    "    with open(latency_path, \"r\") as file:\n",
    "        data = file.readlines()\n",
    "\n",
    "    event_det_lat = []\n",
    "    pattern_det_lat = []\n",
    "    total_latency = []\n",
    "\n",
    "    # Regular expression pattern to extract numerical values from the lines surrounded by $\n",
    "    pattern = r\"\\$(.*?)\\$\"\n",
    "\n",
    "    for line in data:\n",
    "        values = re.findall(pattern, line)\n",
    "\n",
    "        if len(values) == 3:\n",
    "            event_det_lat.append(int(values[0]))\n",
    "            pattern_det_lat.append(int(values[1]))\n",
    "            total_latency.append(int(values[2]))\n",
    "        else:\n",
    "            print(\"expeted three columns!\")\n",
    "\n",
    "    with open(result_path, \"r\") as file:\n",
    "        data = file.readlines()\n",
    "\n",
    "    result_tuples = []\n",
    "    for line in data:\n",
    "        result_tuples.append(line)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"tuple\": result_tuples,\n",
    "        \"event detection latency\": event_det_lat,\n",
    "        \"pattern detection latency\": pattern_det_lat,\n",
    "        \"total latency\": total_latency\n",
    "    })\n",
    "    \n",
    "    #to get unique tuples, it takes mean, but other possibility: take first occurence\n",
    "    grouped_df = df.groupby(\"tuple\").agg(\n",
    "        event_det_lat=(\"event detection latency\", \"mean\"),\n",
    "        pattern_det_lat=(\"pattern detection latency\", \"mean\"),\n",
    "        total_latency=(\"total latency\", \"mean\")).reset_index()\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "398dc76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_det_latency(latency_query_unique):\n",
    "    event_det_lat_sum = latency_query_unique[\"event_det_lat\"].mean()\n",
    "    pattern_det_lat_sum = latency_query_unique[\"pattern_det_lat\"].mean()\n",
    "    total_latency_sum = latency_query_unique[\"total_latency\"].mean()\n",
    "    return np.array([event_det_lat_sum, pattern_det_lat_sum, total_latency_sum])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699979df",
   "metadata": {},
   "source": [
    "### Merged processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c93e92a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for \"selecivity\" experiments:\n",
    "# merges all information into one dataframe, result columns:\n",
    "# Elements/Second, Latency, System, Pattern, Selectivity\n",
    "def get_merged_df_per_pattern_sel(path, sel_folders):\n",
    "    merge_df = pd.DataFrame()\n",
    "    for sel in sel_folders:\n",
    "        file_path = path+\"/\"+sel\n",
    "        merge_df2 = pd.DataFrame()\n",
    "        for file_name in os.listdir(file_path):\n",
    "            if os.path.isfile(os.path.join(file_path, file_name)):\n",
    "                file = file_path + \"/\" + file_name\n",
    "                tput_df = get_throughput_df_from_log(file)#.rename(columns={\"Elements/Second\": \"Throughput\"})\n",
    "                tput_df = tput_df[\"Elements/Second\"]/1000 #display for k elems\n",
    "                lat_df = get_latency_df_from_log(file)\n",
    "                latency = lat_df[\"total_latency\"]/lat_df[\"patterns_cnt\"]\n",
    "                if latency.mean()>10000: #something went wrong\n",
    "                    lat_df[\"Latency\"] = np.nan\n",
    "                else:\n",
    "                    lat_df[\"Latency\"] = latency\n",
    "                settings = get_settings_from_log(file)\n",
    "                system, pattern = settings[\"name\"].split(\"_\")\n",
    "                df = pd.concat([tput_df, lat_df], axis=1)[[\"Elements/Second\", \"Latency\"]].assign(System=system, Pattern=pattern)\n",
    "                merge_df2 = merge_df2.append(df)\n",
    "        merge_df2 = merge_df2.assign(Selectivity=sel)\n",
    "        merge_df = merge_df.append(merge_df2)\n",
    "    \n",
    "    return merge_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bfe55e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for \"all\" experiments:\n",
    "# merges all information into one dataframe, result columns:\n",
    "# Elements/Second, Latency, System, Pattern\n",
    "def get_merged_df_all(path, folders):\n",
    "    merge_df = pd.DataFrame()\n",
    "    folder_is_ingestion_rate=False\n",
    "    if folders[0]==\"200k\": #very hard coded, change for other labbeling or other start rate\n",
    "        folder_is_ingestion_rate=True\n",
    "    for folder in folders:\n",
    "        file_path = path+\"/\"+folder\n",
    "        merge_df2 = pd.DataFrame()\n",
    "        for file_name in os.listdir(file_path):\n",
    "            if os.path.isfile(os.path.join(file_path, file_name)):\n",
    "                file = file_path + \"/\" + file_name\n",
    "                tput_df = get_throughput_df_from_log(file)#.rename(columns={\"Elements/Second\": \"Throughput\"})\n",
    "                tput_df = tput_df[\"Elements/Second\"]/1000\n",
    "                lat_df = get_latency_df_from_log(file)\n",
    "                latency = lat_df[\"total_latency\"]/lat_df[\"patterns_cnt\"]\n",
    "                if latency.mean()>10000: #something went wrong\n",
    "                    lat_df[\"Latency\"] = np.nan\n",
    "                else:\n",
    "                    lat_df[\"Latency\"] = latency\n",
    "                settings = get_settings_from_log(file)\n",
    "                system, pattern = settings[\"name\"].split(\"_\")\n",
    "                df = pd.concat([tput_df, lat_df], axis=1)[[\"Elements/Second\", \"Latency\"]].assign(System=system, Pattern=pattern)\n",
    "                if folder_is_ingestion_rate:\n",
    "                    df[\"ingestion_rate\"] = folder\n",
    "                merge_df2 = merge_df2.append(df)\n",
    "        merge_df = merge_df.append(merge_df2)\n",
    "    \n",
    "    return merge_df.reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
